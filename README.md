# KV-eval-dataset

This repository provides two publicly available datasets as well as our dataset for evaluating the fact checking capability in a knowledge graph.

## Publicly available datasets

* **The Synthetic dataset (synt.tsv)** comprises true triples manually extracted from Wikipedia tables, and false triples automatically generated by negative sampling.

* **The Real-World dataset (real.tsv)** derived from [Google Relation Extraction Corpora](https://ai.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html/) and [WSDM Cup Triple Scoring Challenge](https://www.wsdm-cup-2017.org/triple-scoring.html/), which comprises true triples manually extracted from Wikipedia texts by crowdsourcing, and false triples automatically generated by negative sampling.

## Issues in the publicly available datasets

1. False-labeled fact triples in these datasets are automatically generated by negative sampling.

Therefore, these datasets contain false-labeled true triples (false negatives) that is probable to make an evaluation inaccurate. 

According to our analysis, at least 4% of false triples in these datasets are false negatives.

2. Some true-labeled fact triples in these datasets are already contained in the existing knowledge graph, English DBpedia.

This means that some test cases in these datasets can be easily solved by checking whether a given triple is contained in a knowledge graph or not.

According to our analysis, 77.26% of the Synthetic dataset and 8.58% of the Real-World dataset are already contained in [English DBpedia](https://wiki.dbpedia.org/develop/datasets/downloads-2016-10).

## Our dataset

**Our dataset (ours.tsv)** is constructed by manually labeling true or false labels on fact triples extracted from Wikipedia texts by the state-of-the-art [BERT-based relation extractor](https://github.com/machinereading/bert-ko-re/).

Our dataset is constructed to solve the afore- mentioned issues in the two ways, as follows:
* False triples are manually checked to prevent false negatives.
â€¢ True triples are included in our dataset only if the given  triple is not contained in the knowledge graph, [K-Box](http://kbox.kaist.ac.kr).

These makes our dataset more reasonable and challenging to evaluate the fact checking capability on newly found triples missing in a knowledge graph than the publicly available datasets. 

The statistics of all the datasets in this repository is as follows:

<img src="./images/figure-1.png" width="80%" height="80%">